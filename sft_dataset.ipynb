{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from openai.types.chat.chat_completion_message import ChatCompletionMessage\n",
    "from openai import OpenAI\n",
    "from datasets import load_from_disk, Dataset\n",
    "from document_store import DocumentStore, E5EmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from time import time\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('dataset_curated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': '0013205',\n",
       " 'document_source': 'MPlusHealthTopics',\n",
       " 'document_url': 'https://www.nlm.nih.gov/medlineplus/a1c.html',\n",
       " 'category': 'Other',\n",
       " 'umls_cui': None,\n",
       " 'umls_semantic_types': None,\n",
       " 'umls_semantic_group': None,\n",
       " 'synonyms': 'Glycohemoglobin|HbA1C|Hemoglobin A1C test',\n",
       " 'question_id': '0000001-1',\n",
       " 'question_focus': 'A1C',\n",
       " 'question_type': 'information',\n",
       " 'question': 'Do you have information about A1C',\n",
       " 'answer': 'Summary : A1C is a blood test for type 2 diabetes and prediabetes. It measures your average blood glucose, or blood sugar, level over the past 3 months. Doctors may use the A1C alone or in combination with other diabetes tests to make a diagnosis. They also use the A1C to see how well you are managing your diabetes. This test is different from the blood sugar checks that people with diabetes do every day.    Your A1C test result is given in percentages. The higher the percentage, the higher your blood sugar levels have been:       - A normal A1C level is below 5.7 percent    - Prediabetes is between 5.7 to 6.4 percent. Having prediabetes is a risk factor for getting type 2 diabetes. People with prediabetes may need retests every year.    - Type 2 diabetes is above 6.5 percent    - If you have diabetes, you should have the A1C test at least twice a year. The A1C goal for many people with diabetes is below 7. It may be different for you. Ask what your goal should be. If your A1C result is too high, you may need to change your diabetes care plan.       NIH: National Institute of Diabetes and Digestive and Kidney Diseases',\n",
       " 'document_content': '\\n                        A1C\\n                    \\nAlso called: Glycohemoglobin, HbA1C, Hemoglobin A1C test\\nSummary\\nA1C is a blood test for type 2 diabetes and prediabetes. It measures your average blood glucose, or blood sugar, level over the past 3 months. Doctors may use the A1C alone or in combination with other diabetes tests to make a diagnosis. They also use the A1C to see how well you are managing your diabetes. This test is different from the blood sugar checks that people with diabetes do every day.\\nYour A1C test result is given in percentages. The higher the percentage, the higher your blood sugar levels have been:\\nA normal A1C level is below 5.7%\\nPrediabetes is between 5.7 to 6.4%. Having prediabetes is a risk factor for getting type 2 diabetes. People with prediabetes may need retests every year.\\nType 2 diabetes is above 6.5%\\nIf you have diabetes, you should have the A1C test at least twice a year. The A1C goal for many people with diabetes is below 7. It may be different for you. Ask what your goal should be. If your A1C result is too high, you may need to change your diabetes care plan.\\nNIH: National Institute of Diabetes and Digestive and Kidney Diseases\\nLearn More\\n                                            \\n                                            \\nA1C and eAG\\n (American Diabetes Association)\\nA1C Test and Diabetes\\n (National Institute of Diabetes and Digestive and Kidney Diseases)\\n Also in Spanish \\nA1C Test and Race/Ethnicity\\n (National Institute of Diabetes and Digestive and Kidney Diseases)\\nHemoglobin A1C (HbA1c) Test\\n (National Library of Medicine)\\n Also in Spanish \\nUnderstanding A1C\\n (American Diabetes Association)\\nClinical Trials\\n                                            \\n                                            \\nClinicalTrials.gov: Hemoglobin A1C\\n (National Institutes of Health)\\nJournal Articles\\n                                             \\n                                            References and abstracts from MEDLINE/PubMed (National Library of Medicine)\\nArticle: Impacts of Diabetes Self-Management Education and Support Programs in Hispanic Church...\\nArticle: Comparative efficacy and safety of sitagliptin or gliclazide combined with metformin...\\nArticle: Safety and effectiveness of dual therapy for Helicobacter pylori infection and...\\nA1C -- see more articles\\nPatient Handouts\\n                                            \\n                                            \\nA1C test\\n (Medical Encyclopedia)\\n Also in Spanish \\nTopic Image\\nNational Institutes of Health\\n                              \\n                                The primary NIH organization for research on\\n                                A1C is the\\n                                National Institute of Diabetes and Digestive and Kidney Diseases\\nDisclaimers\\n                                \\n                                MedlinePlus links to health information from the National Institutes of Health and other federal government agencies. MedlinePlus also links to health information from non-government Web sites. See our disclaimer about external links and our quality guidelines.\\n                                \\n                            \\n',\n",
       " 'chunks': ['Category: Other | Synonyms: G, l, y\\n\\nA1C\\n                    \\nAlso called: Glycohemoglobin, HbA1C, Hemoglobin A1C test\\nSummary\\nA1C is a blood test for type 2 diabetes and prediabetes. It measures your average blood glucose, or blood sugar, level over the past 3 months. Doctors may use the A1C alone or in combination with other diabetes tests to make a diagnosis. They also use the A1C to see how well you are managing your diabetes. This test is different from the blood sugar checks that people with diabetes do every day. Your A1C test result is given in percentages. The higher the percentage, the higher your blood sugar levels have been:\\nA normal A1C level is below 5.7%\\nPrediabetes is between 5.7 to 6.4%. Having prediabetes is a risk factor for getting type 2 diabetes. People with prediabetes may need retests every year. Type 2 diabetes is above 6.5%\\nIf you have diabetes, you should have the A1C test at least twice a year. The A1C goal for many people with diabetes is below 7. It may be different for you. Ask what your goal should be. If your A1C result is too high, you may need to change your diabetes care plan. NIH: National Institute of Diabetes and Digestive and Kidney Diseases\\nLearn More\\n                                            \\n                                            \\nA1C and eAG\\n (American Diabetes Association)\\nA1C Test and Diabetes\\n (National Institute of Diabetes and Digestive and Kidney Diseases)\\n Also in Spanish \\nA1C Test and Race/Ethnicity\\n (National Institute of Diabetes and Digestive and Kidney Diseases)\\nHemoglobin A1C (HbA1c) Test\\n (National Library of Medicine)\\n Also in Spanish \\nUnderstanding A1C\\n (American Diabetes Association)\\nClinical Trials\\n                                            \\n                                            \\nClinicalTrials.gov: Hemoglobin A1C\\n (National Institutes of Health)\\nJournal Articles\\n                                             \\n                                            References and abstracts from MEDLINE/PubMed (National Library of Medicine)\\nArticle: Impacts of Diabetes Self-Management Education and Support Programs in Hispanic Church...',\n",
       "  'Category: Other | Synonyms: G, l, y\\n\\nArticle: Comparative efficacy and safety of sitagliptin or gliclazide combined with metformin... Article: Safety and effectiveness of dual therapy for Helicobacter pylori infection and...\\nA1C -- see more articles\\nPatient Handouts\\n                                            \\n                                            \\nA1C test\\n (Medical Encyclopedia)\\n Also in Spanish \\nTopic Image\\nNational Institutes of Health\\n                              \\n                                The primary NIH organization for research on\\n                                A1C is the\\n                                National Institute of Diabetes and Digestive and Kidney Diseases\\nDisclaimers\\n                                \\n                                MedlinePlus links to health information from the National Institutes of Health and other federal government agencies. MedlinePlus also links to health information from non-government Web sites. See our disclaimer about external links and our quality guidelines.'],\n",
       " 'chunk_ids': ['0013205_chunk_0', '0013205_chunk_1']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get('FIREWORKS_API_KEY'),\n",
    "    base_url=\"https://api.fireworks.ai/inference/v1\"\n",
    ")\n",
    "\n",
    "tools = [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search\",\n",
    "                \"description\": \"Search documents in the document store with a detailed long query.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\"type\": \"string\", \"description\": \"Query to search documents\"},\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "\n",
    "doc_store = DocumentStore(\n",
    "    collection_name=\"documents\",\n",
    "    chroma_db_path=\"./chroma_db\",\n",
    "    bm25_index_path=\"./bm25_index\",\n",
    "    document_content_dir=\"./document_content\",\n",
    "    device=\"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(prompt):\n",
    "    \"\"\"Get response from strong model with tool calls\"\"\"\n",
    "    try:\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                'role': \"user\",\n",
    "                'content': prompt\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Allow up to 5 conversation turns (same as llm_rollout)\n",
    "        for _ in range(5):\n",
    "            if isinstance(messages[-1], ChatCompletionMessage) and messages[-1].tool_calls:\n",
    "                # Handle tool calls (document search)\n",
    "                messages.append({\n",
    "                    'role': 'tool',\n",
    "                    'tool_call_id':messages[-1].tool_calls[0].id,\n",
    "                    'content': json.dumps(\n",
    "                        doc_store.search(\n",
    "                            **json.loads(messages[-1].tool_calls[0].function.arguments),\n",
    "                            n_results=5\n",
    "                        )\n",
    "                    )\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            if isinstance(messages[-1], dict):\n",
    "                # Get model response\n",
    "                chat_completion = client.chat.completions.create(\n",
    "                    model=\"accounts/fireworks/models/deepseek-v3\",  # Using strong model for distillation\n",
    "                    messages=messages,\n",
    "                    tools=tools,\n",
    "                )\n",
    "                response = chat_completion.choices[0].message\n",
    "                messages.append(response)\n",
    "                continue\n",
    "        \n",
    "        return messages  # Return full conversation history like llm_rollout\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        print(f\"Error getting model response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = get_model_response(dataset['train'][0][\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 872it [3:39:44, 15.12s/it] \n"
     ]
    }
   ],
   "source": [
    "rollout_results = []\n",
    "\n",
    "for index, example in tqdm(enumerate(dataset['train']), desc=\"Processing examples\"):\n",
    "    if index < 534:\n",
    "        continue\n",
    "    prompt = example[\"question\"]\n",
    "    \n",
    "    # Get model response\n",
    "    time_start = time()\n",
    "    messages = get_model_response(prompt)\n",
    "    if messages is None:\n",
    "        print(index)\n",
    "        continue\n",
    "        \n",
    "    rollout_results.append({\n",
    "        'example': example,\n",
    "        'messages': messages,\n",
    "        'time_elapsed': time()-time_start\n",
    "    })\n",
    "\n",
    "    if (index % 100 == 0 and index > 0) or index == len(dataset['train']) - 1:\n",
    "        with open(f'sft_rollout/{index}.pkl', 'wb') as f:\n",
    "            pickle.dump(rollout_results, f)\n",
    "            rollout_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rollout_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "67\n",
      "100\n",
      "34\n",
      "71\n",
      "100\n",
      "100\n",
      "100\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "all_rollout_result = []\n",
    "\n",
    "for file in os.listdir(\"sft_rollout\"):\n",
    "    with open(os.path.join(\"sft_rollout\", file), 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "        print(len(result))\n",
    "        all_rollout_result.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate \n",
    "all_rollout_result = {question: result for question, result in zip([element['example']['question'] for element in all_rollout_result], all_rollout_result)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rollout_result = list(all_rollout_result.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_rollout import get_reward_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_functions = get_reward_functions(doc_store.embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    rewards = {}\n",
    "    for name, func in reward_functions.items():\n",
    "        rewards[name] = func(example['example'], example['messages'])\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Reward: 100%|██████████| 772/772 [00:35<00:00, 21.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for example in tqdm(all_rollout_result, desc=\"Computing Reward\"):\n",
    "    example['reward'] = process_example(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = [a for a in all_rollout_result if a['reward']['mrr'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_fix(example):\n",
    "    messages = example['messages']\n",
    "    for message in messages:\n",
    "        if isinstance(message, dict) and message.get('role') == 'tool':\n",
    "            content = json.loads(message['content'])\n",
    "            content['ids'] = content['ids'][:5]\n",
    "            content['documents'] = content['documents'][:5]\n",
    "            message['content'] = json.dumps(content)\n",
    "    example['messages'] = messages\n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = [hot_fix(example) for example in sft_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = [example for example in sft_dataset if example['reward']['mrr'] >= 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sft_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama-3.2-3B-Instruct\", use_fast=True)\n",
    "\n",
    "with open('tool_chat_template_llama3.2_pythonic.jinja', 'r') as f:\n",
    "    chat_template = f.read()\n",
    "\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen2.5-7B-Instruct-bnb-4bit\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages_for_tokenizer(messages):\n",
    "    result_messages = []\n",
    "    for message in messages:\n",
    "        if isinstance(message, dict):\n",
    "            result_messages.append(message)\n",
    "        else:\n",
    "            if message.tool_calls:\n",
    "                result_messages.append({\n",
    "                    'role':'assistant',\n",
    "                    'tool_calls':[\n",
    "                        {\n",
    "                            'function': {\n",
    "                                'arguments': json.loads(tool_call.function.arguments),\n",
    "                                'name': tool_call.function.name\n",
    "                            }\n",
    "                        } for tool_call in message.tool_calls\n",
    "                    ]\n",
    "                })\n",
    "            else:\n",
    "                result_messages.append({\n",
    "                    'role':'assistant',\n",
    "                    'content':message.content\n",
    "                })\n",
    "    return result_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[0;32m----> 2\u001b[0m     format_messages_for_tokenizer(\u001b[43msft_dataset\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      3\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools\n\u001b[1;32m      5\u001b[0m )\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sft_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    format_messages_for_tokenizer(sft_dataset[0]['messages']),\n",
    "    tokenize=False,\n",
    "    tools=tools\n",
    ").split('<|eot_id|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151645, 198]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<|im_end|>', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curate the SFT dataset llama\n",
    "input_ids = []\n",
    "labels = []\n",
    "\n",
    "for example in sft_dataset:\n",
    "    templated_text = tokenizer.apply_chat_template(\n",
    "        format_messages_for_tokenizer(example['messages']),\n",
    "        tokenize=False,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    templated_text_splitted = [x + '<|eot_id|>' for x in templated_text.split('<|eot_id|>') if x != '\\n']\n",
    "\n",
    "    _input_ids = []\n",
    "    _lab = []\n",
    "\n",
    "    for t in templated_text_splitted:\n",
    "        tokens = tokenizer.encode(t, add_special_tokens=False)\n",
    "        _input_ids.extend(tokens)\n",
    "        if '' in t or '<|im_start|>user\\n<tool_response>' in t:\n",
    "            _completion_masks.extend([0]*len(tokens))\n",
    "        else:\n",
    "            _completion_masks.extend([1]*len(tokens))\n",
    "    \n",
    "    input_ids.append(_input_ids)\n",
    "    completion_masks.append(_completion_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 164.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# curate the SFT dataset qwen\n",
    "input_ids = []\n",
    "labels = []\n",
    "\n",
    "for example in tqdm(sft_dataset):\n",
    "    templated_text = tokenizer.apply_chat_template(\n",
    "        format_messages_for_tokenizer(example['messages']),\n",
    "        tokenize=False,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    templated_text_splitted = [x + '<|im_end|>' for x in templated_text.split('<|im_end|>') if x != '\\n']\n",
    "\n",
    "    _input_ids = []\n",
    "    _completion_masks = []\n",
    "\n",
    "    for t in templated_text_splitted:\n",
    "        tokens = tokenizer.encode(t, add_special_tokens=False)\n",
    "        _input_ids.extend(tokens)\n",
    "        if '<|im_start|>system' in t or '<|im_start|>user\\n<tool_response>' in t:\n",
    "            _completion_masks.extend([0]*len(tokens))\n",
    "        else:\n",
    "            _completion_masks.extend([1]*len(tokens))\n",
    "    \n",
    "    _labels = _input_ids[1:] + [-100]\n",
    "    for index, mask in enumerate(_completion_masks):\n",
    "        if mask == 0:\n",
    "            _labels[index] = -100\n",
    "    \n",
    "    input_ids.append(_input_ids)\n",
    "    labels.append(_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    format_messages_for_tokenizer(sft_dataset[0]['messages']),\n",
    "    tools=tools\n",
    ")[:-1] == input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1cd3c298414cfcbc0856acb723bc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dataset.from_dict(\n",
    "    {\n",
    "        'input_ids':input_ids,\n",
    "        'labels':labels\n",
    "    }\n",
    ").save_to_disk('sft_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('sft_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['completion_masks'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, TaskType, get_peft_model\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b4cdd828304d1db7c2774567f9edc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Llama-3.2-3B-Instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,156,928 || all params: 3,224,906,752 || trainable%: 0.3770\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Get PEFT model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84306ed286884c72b97c0bf54a305ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained('Llama-3.2-3B-Instruct-lora', is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,156,928 || all params: 3,224,906,752 || trainable%: 0.3770\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('sft_dataset').with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [tensor([128000, 128006,   9125,  ...,  32653,     13, 128009]),\n",
       "  tensor([128000, 128006,   9125,  ...,   1862,     13, 128009])],\n",
       " 'completion_masks': [tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "  tensor([0, 0, 0,  ..., 1, 1, 1])]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 1.8155,  1.9450,  7.3831,  ..., -1.2538, -1.2538, -1.2535],\n",
       "         [-8.8965, -6.4822, -5.3258,  ...,  9.5690,  9.5682,  9.5681],\n",
       "         [ 4.3489,  2.7922,  3.8053,  ..., -1.4698, -1.4689, -1.4697],\n",
       "         ...,\n",
       "         [19.1167,  5.4830,  2.1013,  ...,  2.0884,  2.0887,  2.0883],\n",
       "         [ 3.9380, -1.7175,  1.4737,  ...,  3.5807,  3.5820,  3.5811],\n",
       "         [-2.8495, -1.6710, -3.4438,  ...,  7.2351,  7.2344,  7.2331]]]), past_key_values=DynamicCache(), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=dataset[0]['input_ids'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [[randint(0, 128000) for _ in range(10000)] for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49fb49f864e48f8a6af22d064c96f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dataset.from_dict({\n",
    "    'input_ids': input_ids,\n",
    "    'labels': [input_id[:-1] + [-100] for input_id in input_ids]\n",
    "}).save_to_disk('dummy_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
